{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b493570e-37b2-4c71-8bc9-8477fee59ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "from numba import njit, prange\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import cond, matrix_rank\n",
    "from scipy.sparse import lil_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0edef0-3514-4988-8bf5-63406882499a",
   "metadata": {},
   "source": [
    "## Aggregation of Final Matrix and Symmetry\n",
    "There are some adjustments to the final matrix that we perform to make it ready for clustering. Symmetry is applied by sub-matrix for speed purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a97d4-37f9-4ed5-9827-a32d93f268fe",
   "metadata": {},
   "source": [
    "### Open filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d68f0cd-9904-4782-b95d-b51550a9c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the h5 file \n",
    "worm = 'sub-20190929-06'\n",
    "filepath = '/scratch/nar8991/computer_vision'\n",
    "\n",
    "# TO DO - Maybe add logic so it can just take the file from above rather than opening the .h5 file again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adbc06b7-775b-4550-bf56-de4fdc6f2689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recursively load data from an HDF5 file into a Python dictionary\n",
    "def load_dict_from_h5(file_name):\n",
    "    data = {}\n",
    "    \n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_name, 'r') as f:\n",
    "        # Recursively extract the groups and datasets into a dictionary\n",
    "        def read_group(group, data_dict):\n",
    "            for key, value in group.items():\n",
    "                if isinstance(value, h5py.Group):\n",
    "                    # If it's a group (nested dictionary), recursively read it\n",
    "                    data_dict[key] = {}\n",
    "                    read_group(value, data_dict[key])\n",
    "                else:\n",
    "                    # If it's a dataset, store it in the dictionary\n",
    "                    data_dict[key] = value[()]\n",
    "        \n",
    "        # Start reading the file from the root group\n",
    "        read_group(f, data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load the HDF5 file into a variable\n",
    "label_matrix = load_dict_from_h5(f'{filepath}/{worm}_fast_similarity_matrix.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815377d6-19ec-48d0-a894-ca1afe2d760f",
   "metadata": {},
   "source": [
    "### Functions for symmetry, clustering, and reading in files\n",
    "\n",
    "The label map is used for initial index numbering, which is used for the clustering code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c988aaa8-4f65-465b-8650-31dd5a7e0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_symmetry(M, intended_length):\n",
    "    # Get the number of rows and columns\n",
    "    rows, cols = M.shape\n",
    "    # print(rows, cols)\n",
    "    # If the matrix is not square, pad it with zeros to make it square\n",
    "    if rows != intended_length or cols != intended_length:\n",
    "        padded_matrix = np.zeros((intended_length, intended_length))\n",
    "        padded_matrix[:rows, :cols] = M  # Copy the original matrix into the top-left corner\n",
    "        M = padded_matrix  # Now M is square\n",
    "    \n",
    "    n = intended_length# Now M is guaranteed to be square\n",
    "    # print(n)\n",
    "    # Iterate over the upper triangular part of the matrix (excluding the diagonal)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if M[j, i] == 0 and M[i, j] != 0:\n",
    "                M[j, i] = M[i, j]  # Copy value from upper to lower triangle\n",
    "            elif M[j, i] != 0 and M[i, j] == 0:\n",
    "                M[i, j] = M[j, i]  # Copy value from lower to upper triangle\n",
    "    return M\n",
    "\n",
    "def create_label_map(centroids=40, timepoints=5):\n",
    "    label_map = {}\n",
    "    # Identify timestamps that are non-zero - THIS IS NOT NEEDED FOR OUR ATANAS/EY CODE\n",
    "    for timestamp in range(0, timepoints):\n",
    "        label_map[timestamp] = {}\n",
    "        # Add only the centroids that are in the matrix - THIS IS NEEDED FOR OUR ATANAS/EY CODE\n",
    "        for j in range(0, centroids):\n",
    "            label_map[timestamp][j] = j\n",
    "    return label_map\n",
    "\n",
    "def create_final_matrix(label_dict, centroids=400):\n",
    "    max_t = 0\n",
    "    max_fixed_t = 0\n",
    "    min_non_zero = 1\n",
    "    max_non_zero = 0\n",
    "    sum = 0\n",
    "    total_count = 0\n",
    "    for key, value in label_dict.items():\n",
    "        start = int(key.split('to')[0])\n",
    "        end = int(key.split('to')[1])\n",
    "        non_zero_arr = value[np.nonzero(value)]\n",
    "        max_value = np.max(non_zero_arr)\n",
    "        min_value = np.min(non_zero_arr)\n",
    "        min_non_zero = min(min_value, min_non_zero)\n",
    "        max_non_zero = max(max_value, max_non_zero)\n",
    "        sum += np.sum(non_zero_arr)\n",
    "        total_count += non_zero_arr.size\n",
    "        max_t = max(end, max_t)\n",
    "        max_fixed_t = max(start, max_fixed_t)\n",
    "    print(min_non_zero)\n",
    "    print(max_non_zero)\n",
    "    print(sum/total_count)\n",
    "    dim = (max_t + 1) * centroids  # Assume we start at 0\n",
    "    print(f\"Matrix dimension: {dim}\")\n",
    "    \n",
    "    # Create a sparse matrix in LIL (List of Lists) format for efficient row-based operations\n",
    "    final_matrix = lil_matrix((dim, dim))\n",
    "    \n",
    "    label_map = {}\n",
    "    \n",
    "    for key, value in label_dict.items():\n",
    "        start = int(key.split('to')[0])\n",
    "        end = int(key.split('to')[1])\n",
    "        start_length = value.shape[1]\n",
    "        end_length = value.shape[0]        \n",
    "        \n",
    "        if start not in label_map:\n",
    "            label_map[start] = {}\n",
    "        if start_length-1 not in label_map[start]:\n",
    "            for i in range(start_length):\n",
    "                label_map[start][i] = i\n",
    "        if end not in label_map:\n",
    "            label_map[end] = {}\n",
    "        if end_length-1 not in label_map[end]:\n",
    "            for i in range(end_length):\n",
    "                label_map[end][i] = i\n",
    "        start_val = start * centroids\n",
    "        end_val = end * centroids\n",
    "        \n",
    "        # Set values in the sparse matrix (enforce symmetry if needed)\n",
    "        final_matrix[end_val:end_val+centroids, start_val:start_val+centroids] = -enforce_symmetry(value, centroids)\n",
    "        \n",
    "    return final_matrix, label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c72d1a-69d1-480a-adbf-35096affa031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007535795026375283\n",
      "0.9451219512195121\n",
      "0.043926886872868026\n",
      "Matrix dimension: 384800\n"
     ]
    }
   ],
   "source": [
    "final_matrix, label_map = create_final_matrix(label_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3cb48-c22f-43ec-bfc5-4167e43a1140",
   "metadata": {},
   "source": [
    "## Clustering (Nalini and Maren)\n",
    "\n",
    "We integrated the sparse implementation of hierarchical minimum linkage clustering outlined in the ANTSUN pipeline, converted from Python to Julia. The major change is that we get modify the overlapping timestamp logic to reflect our new data structure.\n",
    "\n",
    "We also adjust the height threshold based on our new data distribution (as well as the fact that we only use IoU and do not apply heuristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f370e-d498-4adf-99c3-6c875b4ed4c3",
   "metadata": {},
   "source": [
    "### SparseClustering.jl functions\n",
    "\n",
    "Recreation of sparse clustering in Python based on code from https://github.com/flavell-lab/SparseClustering.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dc03dca-fcff-4ee9-8552-26489d733632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/flavell-lab/SparseClustering.jl/blob/main/src/unionfind.jl\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        Initializes a UnionFind data structure for n elements.\n",
    "\n",
    "        Args:\n",
    "        - n (int): The number of elements.\n",
    "        \"\"\"\n",
    "        self.parent = list(range(n))  # Parent array: Each element points to itself initially.\n",
    "        self.rank = [0] * n  # Rank array: Initialize ranks to 0.\n",
    "\n",
    "    def find(self, x):\n",
    "        \"\"\"\n",
    "        Finds the representative (root) of the subset containing x with path compression.\n",
    "\n",
    "        Args:\n",
    "        - x (int): The element whose representative we want to find.\n",
    "\n",
    "        Returns:\n",
    "        - int: The representative (root) of the subset containing x.\n",
    "        \"\"\"\n",
    "        if self.parent[x] != x:\n",
    "            # Path compression: flatten the structure by pointing directly to the root.\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x, y):\n",
    "        \"\"\"\n",
    "        Merges the subsets containing x and y using union by rank.\n",
    "\n",
    "        Args:\n",
    "        - x (int): The first element to merge.\n",
    "        - y (int): The second element to merge.\n",
    "        \"\"\"\n",
    "        rootX = self.find(x)\n",
    "        rootY = self.find(y)\n",
    "\n",
    "        if rootX != rootY:\n",
    "            # Union by rank: Attach the smaller tree to the root of the larger tree\n",
    "            if self.rank[rootX] > self.rank[rootY]:\n",
    "                self.parent[rootY] = rootX\n",
    "            elif self.rank[rootX] < self.rank[rootY]:\n",
    "                self.parent[rootX] = rootY\n",
    "            else:\n",
    "                # If both have the same rank, attach rootY to rootX and increase rank of rootX\n",
    "                self.parent[rootY] = rootX\n",
    "                self.rank[rootX] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d6c76d2-b5dc-4968-8e12-2576677192ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/flavell-lab/SparseClustering.jl/blob/main/src/unionfind.jl\n",
    "\n",
    "class UnionFindAdj:\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        Initializes a UnionFind data structure for n elements.\n",
    "\n",
    "        Args:\n",
    "        - n (int): The number of elements.\n",
    "        \"\"\"\n",
    "        self.parent = list(range(n))  # Parent array: Each element points to itself initially.\n",
    "        self.rank = [0] * n  # Rank array: Initialize ranks to 0.\n",
    "\n",
    "    def find(self, x):\n",
    "        \"\"\"\n",
    "        Finds the representative (root) of the subset containing x with path compression.\n",
    "\n",
    "        Args:\n",
    "        - x (int): The element whose representative we want to find.\n",
    "\n",
    "        Returns:\n",
    "        - int: The representative (root) of the subset containing x.\n",
    "        \"\"\"\n",
    "        if self.parent[x] != x:\n",
    "            # Path compression: flatten the structure by pointing directly to the root.\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x, y,x_ts, y_ts):\n",
    "        \"\"\"\n",
    "        Merges the subsets containing x and y using union by rank.\n",
    "\n",
    "        Args:\n",
    "        - x (int): The first element to merge.\n",
    "        - y (int): The second element to merge.\n",
    "        \"\"\"\n",
    "        rootX = self.find(x)\n",
    "        rootY = self.find(y)\n",
    "\n",
    "        new_ts = set(y_ts).union(set(x_ts))        \n",
    "        \n",
    "        if rootX != rootY:\n",
    "            # Union by rank: Attach the smaller tree to the root of the larger tree\n",
    "            if self.rank[rootX] > self.rank[rootY]:\n",
    "                self.parent[rootY] = rootX\n",
    "            elif self.rank[rootX] < self.rank[rootY]:\n",
    "                self.parent[rootX] = rootY\n",
    "            else:\n",
    "                # If both have the same rank, attach rootY to rootX and increase rank of rootX\n",
    "                self.parent[rootY] = rootX\n",
    "                self.rank[rootX] += 1\n",
    "        return new_ts\n",
    " \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d189e902-0174-4f94-ac2e-5a217222ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/flavell-lab/SparseClustering.jl/blob/main/src/util.jl\n",
    "\n",
    "def sort_distance_matrix(s):\n",
    "    \"\"\"\n",
    "    Sort the non-zero entries of the upper triangular part of a sparse distance matrix\n",
    "    and return the (i, j) indices of these entries sorted by their values.\n",
    "    \n",
    "    Arguments:\n",
    "    - s (scipy.sparse.csr_matrix): A sparse matrix, typically representing a distance matrix.\n",
    "    \n",
    "    Returns:\n",
    "    - List of tuples: A list of (i, j) index tuples sorted by the distance values.\n",
    "    \"\"\"\n",
    "    coo = s.tocoo()\n",
    "    pairs = []\n",
    "    for i, j, value in zip(coo.row, coo.col, coo.data):\n",
    "        if i < j:  # Only consider the upper triangular part (excluding the diagonal)\n",
    "            pairs.append((i, j, value))\n",
    "    \n",
    "    pairs.sort(key=lambda x: x[2])\n",
    "    return [(i, j) for i, j, _ in pairs]\n",
    "\n",
    "\n",
    "def generate_timepoint_map(inv_map, n, max_timept):\n",
    "    \"\"\"\n",
    "    Generate a matrix representing time points where each ROI was detected.\n",
    "    \n",
    "    Arguments:\n",
    "    - inv_map (dict): A dictionary where each key is an ROI, and each value is a set of time points where the ROI was found.\n",
    "    - n (int): Number of ROIs.\n",
    "    - max_timept (int): The maximum number of time points in the dataset.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.ndarray: A matrix of size n x max_timept. The entry (i, j) is 1 if the i-th ROI was detected at time point j, 0 otherwise.\n",
    "    \"\"\"\n",
    "    # I added another point because \"max timept\" is not actually max timept\n",
    "    timepoint_map = np.zeros((n, max_timept), dtype=int)\n",
    "    \n",
    "    for roi, timepoints in inv_map.items():\n",
    "        for t in timepoints:\n",
    "            timepoint_map[roi, t] = 1\n",
    "    \n",
    "    return timepoint_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec0286-4784-4641-a8a9-08767e513209",
   "metadata": {},
   "source": [
    "No timestep-merge clustering which was not effective and thus not used, but we included the accompanying code given related visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c47eba17-6c33-4b31-9d86-fe8a80618e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/flavell-lab/SparseClustering.jl/blob/main/src/hclust.jl\n",
    "\n",
    "def hclust_minimum_threshold_sparse(ds, inv_map, overlap_threshold, height_threshold, use_sparse=True, pair_match=False):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering on a pairwise distance matrix with additional constraints:\n",
    "    - Avoid merging clusters that would cause ROIs from the same time point to be clustered together.\n",
    "    - Stop clustering when the distance exceeds the height threshold.\n",
    "    \n",
    "    Arguments:\n",
    "    - ds (numpy.ndarray or scipy.sparse matrix): Matrix of pairwise distances between ROIs.\n",
    "    - inv_map (dict): Mapping from ROIs to time points.\n",
    "    - overlap_threshold (float): Threshold for ROI overlap in clusters (0 to 1).\n",
    "    - height_threshold (float): Maximum distance for merging ROIs.\n",
    "    - use_sparse (bool): Whether to use sparse matrices for the distance matrix.\n",
    "    - pair_match (bool): Whether to restrict merges to pairs of size at most 2.\n",
    "    \n",
    "    Returns:\n",
    "    - UnionFind: A UnionFind object representing the current clusters.\n",
    "    \"\"\"\n",
    "    # Convert to sparse if needed\n",
    "    if use_sparse:\n",
    "        d = ds.tocsr()\n",
    "    else:\n",
    "        d = np.array(ds)\n",
    "\n",
    "    n = d.shape[0]\n",
    "    \n",
    "    # Sort the distance matrix (upper triangle)\n",
    "    sorted_pairs_list = sort_distance_matrix(d)\n",
    "    \n",
    "    # Generate timepoint map\n",
    "    max_frame = max(max(y for y in timepoints) for timepoints in inv_map.values()) + 1\n",
    "\n",
    "    ROI_num = int(n / max_frame)\n",
    "    # print(ROI_num)\n",
    "    \n",
    "    timepoint_map = generate_timepoint_map(inv_map, ROI_num, max_frame)\n",
    "\n",
    "    # Initialize UnionFind for clustering\n",
    "    curr_cluster_ids = UnionFind(n)\n",
    "\n",
    "    # Initialize merged nodes tracking if pair_match is True\n",
    "    if pair_match:\n",
    "        merged_nodes = [False] * n\n",
    "    \n",
    "    new_tree_frame = np.zeros_like(timepoint_map[1, :], dtype=int)\n",
    "\n",
    "    # print(sorted_pairs_list)\n",
    "    for i, j in sorted_pairs_list:\n",
    "        # If the distance is too high, stop clustering\n",
    "        # print(d[i,j])\n",
    "        if d[i, j] > height_threshold:\n",
    "            break\n",
    "        # print(\"Passed height threshold\")\n",
    "\n",
    "        clust_i = curr_cluster_ids.find(i)\n",
    "        clust_j = curr_cluster_ids.find(j)\n",
    "        # print(clust_i)\n",
    "        # print(clust_j)\n",
    "\n",
    "        # Skip if they are already in the same cluster or already merged if pair_match\n",
    "        if clust_i == clust_j or (pair_match and (merged_nodes[i] or merged_nodes[j])):\n",
    "            continue\n",
    "        i_timestamp = clust_i // ROI_num\n",
    "        j_timestamp = clust_j // ROI_num\n",
    "\n",
    "        # # This is only if the centroids \n",
    "        # if i_timestamp == j_timestamp:\n",
    "        #     continue\n",
    "\n",
    "        curr_cluster_ids.union(clust_i, clust_j)\n",
    "        \n",
    "        overlaps = np.sum(new_tree_frame > 1)\n",
    "\n",
    "        # If pair_match is True, mark the nodes as merged\n",
    "        if pair_match:\n",
    "            merged_nodes[i] = True\n",
    "            merged_nodes[j] = True\n",
    "\n",
    "    return curr_cluster_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17cb0359-fb68-48c4-a562-d124479024f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/flavell-lab/SparseClustering.jl/blob/main/src/hclust.jl\n",
    "\n",
    "def hclust_minimum_threshold_sparse_no_timestep(ds, inv_map, overlap_threshold, height_threshold, use_sparse=True, pair_match=False):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering on a pairwise distance matrix with additional constraints:\n",
    "    - Avoid merging clusters that would cause ROIs from the same time point to be clustered together.\n",
    "    - Stop clustering when the distance exceeds the height threshold.\n",
    "    \n",
    "    Arguments:\n",
    "    - ds (numpy.ndarray or scipy.sparse matrix): Matrix of pairwise distances between ROIs.\n",
    "    - inv_map (dict): Mapping from ROIs to time points.\n",
    "    - overlap_threshold (float): Threshold for ROI overlap in clusters (0 to 1).\n",
    "    - height_threshold (float): Maximum distance for merging ROIs.\n",
    "    - use_sparse (bool): Whether to use sparse matrices for the distance matrix.\n",
    "    - pair_match (bool): Whether to restrict merges to pairs of size at most 2.\n",
    "    \n",
    "    Returns:\n",
    "    - UnionFind: A UnionFind object representing the current clusters.\n",
    "    \"\"\"\n",
    "    # Convert to sparse if needed\n",
    "    if use_sparse:\n",
    "        d = ds.tocsr()\n",
    "    else:\n",
    "        d = np.array(ds)\n",
    "\n",
    "    n = d.shape[0]\n",
    "    \n",
    "    # Sort the distance matrix (upper triangle)\n",
    "    sorted_pairs_list = sort_distance_matrix(d)\n",
    "    \n",
    "    # Generate timepoint map\n",
    "    max_frame = max(max(y for y in timepoints) for timepoints in inv_map.values()) + 1\n",
    "    ROI_num = int(n / max_frame)\n",
    "\n",
    "    # print(ROI_num)\n",
    "    \n",
    "    timepoint_map = generate_timepoint_map(inv_map, ROI_num, max_frame)\n",
    "\n",
    "    # Initialize UnionFind for clustering\n",
    "    curr_cluster_ids = UnionFindAdj(n)\n",
    "\n",
    "    # Initialize merged nodes tracking if pair_match is True\n",
    "    if pair_match:\n",
    "        merged_nodes = [False] * n\n",
    "    \n",
    "    new_tree_frame = np.zeros_like(timepoint_map[1, :], dtype=int)\n",
    "\n",
    "    timesteps_per_cluster=[]\n",
    "    for i in np.arange(n):\n",
    "        timesteps_per_cluster.append([i//ROI_num])\n",
    "\n",
    "    # print(sorted_pairs_list)\n",
    "    for i, j in sorted_pairs_list:\n",
    "        # If the distance is too high, stop clustering\n",
    "        # print(d[i,j])\n",
    "        if d[i, j] > height_threshold:\n",
    "            break\n",
    "        # print(\"Passed height threshold\")\n",
    "\n",
    "        clust_i = curr_cluster_ids.find(i)\n",
    "        clust_j = curr_cluster_ids.find(j)\n",
    "        # print(clust_i)\n",
    "        # print(clust_j)\n",
    "\n",
    "        # Skip if they are already in the same cluster or already merged if pair_match\n",
    "        if clust_i == clust_j or (pair_match and (merged_nodes[i] or merged_nodes[j])):\n",
    "            continue\n",
    "            \n",
    "        i_timestamp = clust_i // ROI_num\n",
    "        j_timestamp = clust_j // ROI_num\n",
    "        \n",
    "        if i_timestamp == j_timestamp:\n",
    "            continue\n",
    "\n",
    "        if (j_timestamp in timesteps_per_cluster[clust_i]) or (i_timestamp in timesteps_per_cluster[clust_j]):\n",
    "            continue\n",
    "        if set(timesteps_per_cluster[clust_i]).isdisjoint(set(timesteps_per_cluster[clust_j])):\n",
    "            new_ts = curr_cluster_ids.union(clust_i, clust_j, timesteps_per_cluster[clust_i],timesteps_per_cluster[clust_j])\n",
    "            timesteps_per_cluster[clust_i]=new_ts\n",
    "            timesteps_per_cluster[clust_j]=new_ts\n",
    "\n",
    "        # If pair_match is True, mark the nodes as merged\n",
    "        if pair_match:\n",
    "            merged_nodes[i] = True\n",
    "            merged_nodes[j] = True\n",
    "\n",
    "    return curr_cluster_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21b357c-84f7-4baf-a358-6288b10abd60",
   "metadata": {},
   "source": [
    "### ExtractRegisteredData.jl\n",
    "\n",
    "We only need implementation for find_neurons for this: https://github.com/flavell-lab/ExtractRegisteredData.jl/blob/master/src/register_neurons.jl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18cc7e18-3c1c-46f9-ac53-f448880b5716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_label_map(label_map):\n",
    "    inverted_map = defaultdict(lambda: defaultdict(list))  # Nested defaultdict\n",
    "    \n",
    "    # Iterate over the time points and ROIs in the original label_map\n",
    "    for t in label_map:\n",
    "        for roi in label_map[t]:\n",
    "            target_roi = label_map[t][roi]\n",
    "            \n",
    "            # Append the time point `t` to the list of ROIs in the inverted map\n",
    "            inverted_map[target_roi][t].append(roi)\n",
    "    \n",
    "    # Convert defaultdict to a regular dict before returning (optional, for cleaner output)\n",
    "    return {key: value for key, value in inverted_map.items()}\n",
    "\n",
    "def update_label_map(label_map, matches):\n",
    "    \"\"\"\n",
    "    Updates ROI label map `label_map` to include ROI matches `matches`, and returns the updated version.\n",
    "\n",
    "    Arguments:\n",
    "    - label_map (dict): A dictionary mapping time points to another dictionary of ROI labels.\n",
    "    - matches (dict): A dictionary that maps original ROI labels to updated ROI labels.\n",
    "\n",
    "    Returns:\n",
    "    - new_label_map (dict): A dictionary mapping time points to updated ROI labels.\n",
    "    \"\"\"\n",
    "    new_label_map = {}\n",
    "    for t in label_map:\n",
    "        new_label_map[t] = {}\n",
    "        for roi in label_map[t]:\n",
    "            matches_t_roi = t*len(label_map[t]) + roi\n",
    "            initial_mapping = matches[matches_t_roi]\n",
    "            final_mapping = initial_mapping % len(label_map[t])\n",
    "            new_label_map[t][roi] = final_mapping\n",
    "    \n",
    "    return new_label_map\n",
    "\n",
    "def find_neurons(regmap_matrix, label_map, overlap_threshold=0.05, height_threshold=-0.0003, dtype=np.float64, pair_match=False, cluster_lim=False):\n",
    "    \"\"\"\n",
    "    Groups ROIs (Regions of Interest) into neurons based on a matrix of pairwise overlaps and a given label map.\n",
    "\n",
    "    Arguments:\n",
    "    - regmap_matrix (np.ndarray): Matrix representing the pairwise overlaps between the ROIs.\n",
    "    - label_map (dict): A dictionary mapping original ROIs to new ROI labels for each time point.\n",
    "    - overlap_threshold (float): A threshold for the fraction of overlapping ROIs from the same time point. Default is 0.05.\n",
    "    - height_threshold (float): The maximum distance or overlap between two ROIs that can be merged. Default is -0.0003.\n",
    "    - dtype (type): The desired data type for internal computations. Default is np.float64.\n",
    "    - pair_match (bool): A flag indicating whether to merge clusters with a maximum size of 2. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - new_label_map (dict): A dictionary mapping original ROIs to neuron labels for each time point.\n",
    "    - new_inv_map (dict): A dictionary mapping neuron labels back to the original ROIs for each time point.\n",
    "    \"\"\"\n",
    "    # Invert the label map (mapping from ROI to time points)\n",
    "    inv_map = invert_label_map(label_map)\n",
    "    dist = regmap_matrix #Do negative sign in preprocessing instead to avoid overflow\n",
    "\n",
    "    # Perform hierarchical clustering with a minimum threshold\n",
    "    if cluster_lim:\n",
    "        clusters = hclust_minimum_threshold_sparse_no_timestep(dist, inv_map, overlap_threshold, height_threshold, pair_match=pair_match)\n",
    "    else:\n",
    "        clusters = hclust_minimum_threshold_sparse(dist, inv_map, overlap_threshold, height_threshold, pair_match=pair_match)\n",
    "\n",
    "    # Create a dictionary for cluster to ROI mapping -- THIS IS NOT WORKING PROPERLY\n",
    "    n = len(inv_map)\n",
    "    n_to_c_full = []\n",
    "    for i in range(regmap_matrix.shape[0]):\n",
    "        n_to_c_full.append(clusters.find(i))\n",
    "\n",
    "    # Update the label map based on the clustering results\n",
    "    new_label_map = update_label_map(label_map, n_to_c_full)\n",
    "\n",
    "    # Invert the updated label map to get the inverse map\n",
    "    new_inv_map = invert_label_map(new_label_map)\n",
    "\n",
    "    return new_label_map, new_inv_map, n_to_c_full, clusters\n",
    "\n",
    "\n",
    "# Hardcode these parameters\n",
    "def find_neurons_with_params(regmap_matrix, label_map, overlap_threshold=0.05, height_threshold=-0.0003, cluster_lim = False):\n",
    "    \"\"\"\n",
    "    Groups ROIs into neurons based on a matrix of pairwise overlaps and a given label map, using parameters from a dictionary.\n",
    "\n",
    "    Arguments:\n",
    "    - regmap_matrix (np.ndarray): Matrix of pairwise overlaps between the ROIs.\n",
    "    - label_map (dict): Dictionary of dictionaries mapping original ROIs to new ROI labels, for each time point.\n",
    "    - param (dict): Dictionary containing `cluster_overlap_thresh` and `cluster_height_thresh` parameter settings to use for clustering.\n",
    "\n",
    "    Returns:\n",
    "    - new_label_map (dict): Dictionary of dictionaries mapping original ROIs to neuron labels, for each time point.\n",
    "    - inv_map (dict): Dictionary of dictionaries mapping time points to original ROIs, for each neuron label.\n",
    "    \"\"\"\n",
    "    return find_neurons(regmap_matrix, label_map,\n",
    "                        overlap_threshold=overlap_threshold,\n",
    "                        height_threshold=height_threshold, cluster_lim=cluster_lim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d87a5c6-0165-45aa-aef0-3ff36e7f88d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_map, new_inv_map, n_to_c, clusters = find_neurons_with_params(final_matrix, label_map, height_threshold=0, overlap_threshold=2, cluster_lim = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c42d50-dd23-4a83-9e54-f4e038628aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict = {}\n",
    "clusters_set = set()\n",
    "all_values = set(range(len(n_to_c)))\n",
    "for idx, val in enumerate(n_to_c):\n",
    "    if idx != val:\n",
    "        if val not in clusters_dict:\n",
    "            clusters_dict[val] = [idx, val]\n",
    "            clusters_set.add(idx)\n",
    "            clusters_set.add(val)\n",
    "        else:\n",
    "            clusters_dict[val].append(idx)\n",
    "            clusters_set.add(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8423147-0e9d-42d7-822c-393b94dfd3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(final_matrix,markersize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299b11d-c89d-4a4f-a1f8-e7b1ec2c9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict.keys()\n",
    "lens=[]\n",
    "for key, ROIs in clusters_dict.items():\n",
    "    lens.append(len(ROIs))\n",
    "plt.hist(lens,log=True)   \n",
    "plt.savefig(\"roi_cluster_sizes_no_cluster_lim.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b8972-7e46-42c3-82d7-fc8a0c7806b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters = len(clusters_dict)\n",
    "total_timestamps = set()\n",
    "clusters_dict_timestamps = {}\n",
    "for key, value in clusters_dict.items():\n",
    "    clusters_dict_timestamps[key] = set()\n",
    "    for i in value:\n",
    "        timestamp = i // 400\n",
    "        clusters_dict_timestamps[key].add(timestamp)\n",
    "        total_timestamps.add(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c99ca-4681-4259-9b2f-62c865018730",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lengths = [len(value) for value in clusters_dict_timestamps.values()]\n",
    "\n",
    "# Calculate the average and standard deviation\n",
    "average_length = np.mean(set_lengths)\n",
    "std_length = np.std(set_lengths)\n",
    "print(average_length, std_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6198b00-2d0b-4f13-806a-a72ab2a9367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_map, new_inv_map, n_to_c, clusters = find_neurons_with_params(final_matrix, label_map, height_threshold=0, overlap_threshold=2, cluster_lim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be33a4-1aa6-4d00-aa31-1a80ad23bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict = {}\n",
    "clusters_set = set()\n",
    "all_values = set(range(len(n_to_c)))\n",
    "for idx, val in enumerate(n_to_c):\n",
    "    if idx != val:\n",
    "        if val not in clusters_dict:\n",
    "            clusters_dict[val] = [idx, val]\n",
    "            clusters_set.add(idx)\n",
    "            clusters_set.add(val)\n",
    "        else:\n",
    "            clusters_dict[val].append(idx)\n",
    "            clusters_set.add(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42eb2c-0ab9-4e3a-bb64-7e76b34de63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(final_matrix,markersize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80901d18-bc46-4f8b-932d-666e4565970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted=np.argsort(n_to_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb87e7-3430-49cf-adb5-315e62b1853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B = final_matrix[sorted, :][:, sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b059f52d-dcd0-43ef-950a-85ad8a92cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.spy(B,markersize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35dc8b-6d4e-4409-9ba1-ca42c458eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict.keys()\n",
    "lens=[]\n",
    "for key, ROIs in clusters_dict.items():\n",
    "    lens.append(len(ROIs))\n",
    "plt.hist(lens,log=True)   \n",
    "plt.savefig(\"roi_cluster_sizes_no_cluster_lim.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91faae-36ff-4dc7-8d75-2481a0157099",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_remove = all_values - clusters_set\n",
    "new_label_map_adj = new_label_map \n",
    "for value in values_to_remove:\n",
    "    timestamp = value // 400\n",
    "    roi = value % 400\n",
    "    new_label_map_adj[timestamp][roi]=999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd518c7-5daa-4956-8bb2-6b13d47010eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters = len(clusters_dict)\n",
    "total_timestamps = set()\n",
    "clusters_dict_timestamps = {}\n",
    "for key, value in clusters_dict.items():\n",
    "    clusters_dict_timestamps[key] = set()\n",
    "    for i in value:\n",
    "        timestamp = i // 400\n",
    "        clusters_dict_timestamps[key].add(timestamp)\n",
    "        total_timestamps.add(timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f071b20-bb02-4a3a-9b0b-7989c0b39aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lengths = [len(value) for value in clusters_dict_timestamps.values()]\n",
    "\n",
    "# Calculate the average and standard deviation\n",
    "average_length = np.mean(set_lengths)\n",
    "std_length = np.std(set_lengths)\n",
    "print(average_length, std_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7cfbd6-c8fb-4f0d-8547-a2c5f6a026f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = f'{worm}_label_map_20_999.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(new_label_map_adj, f, indent=4)\n",
    "\n",
    "print(f\"saved at {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db510feb-dd7f-4454-b5a6-0041d41a7ea6",
   "metadata": {},
   "source": [
    "## Applying Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b3f6e-3fd4-4036-a8e9-d96441e9a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{filepath}/{worm}_label_map_20_999.json', 'r') as f:\n",
    "    dictionary_data = json.load(f)\n",
    "\n",
    "path=f'/scratch/mie8014/BAN/computer_vision/data/EY/fold1/test/{worm}/moving_rois.h5'\n",
    "moving=h5py.File(path,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70b552-a236-44f3-be45-c41b24b69271",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = np.zeros((len(moving.keys()),256,128, 21))\n",
    "for prob in tqdm(moving.keys()):\n",
    "    mov,fixed=prob.split('to')\n",
    "    t=int(mov)\n",
    "    data = moving[prob]\n",
    "    labels=np.unique(data)[1:] #extract actual ROI labels, skip background\n",
    "    dict= dictionary_data[mov]\n",
    "    for roi in dict.keys():\n",
    "        if dict[roi]==999:\n",
    "            continue\n",
    "        elif int(roi)<len(labels):\n",
    "            new_data[t,:,:,:]=np.where(data[:,:,:]==labels[int(roi)],dict[roi]+1,new_data[t,:,:,:]) #dict[roi]+1 to avoid merging with background for 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5753878-d296-4c6c-b6d0-722b886a27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_centroids(segmentation, unique_labels):\n",
    "    centroids=[]\n",
    "\n",
    "    # unique_labels = np.unique(segmentation)\n",
    "\n",
    "    for label in unique_labels:\n",
    "        if label == 0:  # Skip background label\n",
    "            continue\n",
    "        \n",
    "        # Get the indices (coordinates) where the label exists\n",
    "        coords = np.column_stack(np.where(segmentation == label))\n",
    "        \n",
    "        # Compute the centroid by averaging the coordinates\n",
    "        centroid = np.mean(coords, axis=0)  # axis=0 means average across the (x, y, z) dimensions\n",
    "        \n",
    "        # Append the centroid to the list\n",
    "        centroids.append(centroid)\n",
    "\n",
    "    # Convert the list of centroids into a numpy array (shape will be 113x3)\n",
    "    centroids = np.array(centroids)\n",
    "    return centroids[:400, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234ed2f-8ea6-4002-8ed8-160d818a7d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(new_data) # Check how many neurons across timesteps (tomorrow)\n",
    "final_output = np.zeros((new_data.shape[0], 400, 3))\n",
    "for timestamp in tqdm(range(new_data.shape[0])):\n",
    "    centroid = make_centroids(new_data[timestamp, :, :], unique_labels)\n",
    "    final_output[timestamp, :, :] = centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8a5b8-fbec-4a9e-95fb-9f881932c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(new_data[t,:,:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f543a-ad7d-4ee1-8e07-b2e27b0ca52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save new data (reordered segmentation) as numpy array\n",
    "np.save(f'{worm}_reordered_seg20_centroids', final_output)\n",
    "print(f'{worm}_reordered_seg20_centroids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992ce3c-11bd-43de-828f-6ee3ddf1e8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "julia_works",
   "language": "python",
   "name": "julia_works"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
